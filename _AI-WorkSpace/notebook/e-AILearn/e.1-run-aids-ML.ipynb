{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "881d73c8-f12f-4a9b-a485-996a76289767",
   "metadata": {},
   "source": [
    "# Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b19d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "import logging\n",
    "import random\n",
    "import pandas as pd \n",
    "from pprint import pprint \n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# WorkSpace\n",
    "KEY = 'WorkSpace'; WORKSPACE_PATH = os.getcwd().split(KEY)[0] + KEY; print(WORKSPACE_PATH)\n",
    "os.chdir(WORKSPACE_PATH)\n",
    "sys.path.append(WORKSPACE_PATH)\n",
    "\n",
    "# Pipeline Space\n",
    "from proj_space import SPACE\n",
    "SPACE['WORKSPACE_PATH'] = WORKSPACE_PATH\n",
    "sys.path.append(SPACE['CODE_FN'])\n",
    "pprint(SPACE)\n",
    "\n",
    "# Available Packages\n",
    "import argparse\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from datetime import datetime \n",
    "\n",
    "from recfldtkn.ckpd_obs import Ckpd_ObservationS\n",
    "from recfldtkn.configfn import load_cohort_args\n",
    "from recfldtkn.loadtools import update_args_to_list\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "recfldtkn_config_path = os.path.join(SPACE['CODE_RFT'], 'config_recfldtkn/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c24d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from recfldtkn.loadtools import load_ds_rec_and_info\n",
    "from recfldtkn.configfn import load_cohort_args, load_record_args\n",
    "\n",
    "base_config = load_cohort_args(recfldtkn_config_path, SPACE)\n",
    "print(base_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bc5916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.loadtools import fetch_entry_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876a6cc8",
   "metadata": {},
   "source": [
    "# Step 1: DF Learning Case with Tags/Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a311f4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_learning = True \n",
    "use_inference = not use_learning\n",
    "\n",
    "######################################\n",
    "TriggerCaseMethod = 'TrulicityRx'\n",
    "cohort_label_list = [1]\n",
    "Trigger2LearningMethods = [\n",
    "    {'op':'Tag',    'Name': 'TagPttBasicInfo', 'type': 'learning-only'},\n",
    "    {'op':'Filter', 'Name': 'FilterBasicPRx',  'type': 'learning-only'},\n",
    "]\n",
    "######################################\n",
    "\n",
    "\n",
    "##############\n",
    "RANDOM_SEED = 42\n",
    "downsample_ratio = 1 # 1 (don't drop any case), 0.1 (drop 90% of cases of one patient).\n",
    "out_ratio = 0\n",
    "test_ratio = '2023.10.15'#  'tail0.1'\n",
    "valid_ratio = 0.1 \n",
    "SplitMethod = f'rs{RANDOM_SEED}-ds{downsample_ratio}-out{out_ratio}ts{test_ratio}vd{valid_ratio}' \n",
    "##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c636bced",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.pipeline_model import get_Trigger_Cases, convert_TriggerCases_to_LearningCases\n",
    "from recfldtkn.loadtools import fetch_trigger_tools    \n",
    "\n",
    "Trigger_Tools = fetch_trigger_tools(TriggerCaseMethod, SPACE)\n",
    "RecName_to_dsRec = {}\n",
    "RecName_to_dsRecInfo = {}\n",
    "\n",
    "# dsRec comes from \n",
    "# (1) cohort_label_list, base_config, SPACE\n",
    "# (2) RecName_to_dsRec, RecName_to_dsRecInfo\n",
    "df_case = get_Trigger_Cases(TriggerCaseMethod,\n",
    "                            cohort_label_list,\n",
    "                            base_config, SPACE, \n",
    "                            RecName_to_dsRec, \n",
    "                            RecName_to_dsRecInfo)\n",
    "\n",
    "                            \n",
    "df_case = convert_TriggerCases_to_LearningCases(df_case, \n",
    "                                                cohort_label_list,\n",
    "                                                Trigger2LearningMethods, \n",
    "                                                base_config, \n",
    "                                                use_inference)\n",
    "\n",
    "\n",
    "logger.info(f'After: {df_case.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54f4992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.pipeline_model import assign_caseSplitTag_to_dsCaseLearning\n",
    "\n",
    "\n",
    "df_case = assign_caseSplitTag_to_dsCaseLearning(df_case, \n",
    "                                                RANDOM_SEED, \n",
    "                                                downsample_ratio, out_ratio, \n",
    "                                                test_ratio, valid_ratio)\n",
    "df_case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c276abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# df_case = df_case.sample(2000)\n",
    "###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54cc490",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_case_learning = df_case \n",
    "df_case_learning.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30297381",
   "metadata": {},
   "source": [
    "# Step 2: TrainSet and EvalSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0f18fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Update this part in the future. \n",
    "def get_dfset_from_SetName(df_dsmp, SetName, case_id_columns, SubGroupFilterMethod):\n",
    "    Split, SubGroup = SetName.split(':')\n",
    "    for i in ['In', 'Out', 'Train', 'Valid', 'Test']:\n",
    "        if i.lower() in Split.lower():\n",
    "            df_dsmp = df_dsmp[df_dsmp[i]].reset_index(drop = True)\n",
    "    df_set = df_dsmp[case_id_columns].reset_index(drop = True)\n",
    "    if SubGroup.lower() != 'all':\n",
    "        pass\n",
    "    return df_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e45fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "TrainSetName = 'in_train:all'\n",
    "EvalSetNames = ['in_valid:all', 'in_test:all']\n",
    "#########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2692a264",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_case = df_case_learning\n",
    "case_id_columns = Trigger_Tools['case_id_columns']\n",
    "SubGroupFilterMethod = {}\n",
    "d = {}\n",
    "for SetName in [TrainSetName] + EvalSetNames:\n",
    "    df_set = get_dfset_from_SetName(df_case, SetName, case_id_columns, SubGroupFilterMethod)\n",
    "    ds_set = datasets.Dataset.from_pandas(df_set)\n",
    "    print(SetName, len(df_set))\n",
    "    d[SetName] = ds_set\n",
    "ds_case_dict = datasets.DatasetDict(d)\n",
    "ds_case_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587b444a",
   "metadata": {},
   "source": [
    "# Step 3: Input and Output CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8b2d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.observer import get_CaseFeatInfo_for_a_CaseFeatName\n",
    "from recfldtkn.observer import CaseFeatureTransformer\n",
    "from recfldtkn.observer import get_fn_case_GammaFullInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4f0b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in learning phase.\n",
    "RecName_to_dsRec = {}\n",
    "RecName_to_dsRecInfo = {}\n",
    "use_CF_from_disk = True\n",
    "use_CO_from_disk = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06241c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gamma_Configs = {\n",
    "    'InputX': { \n",
    "        'case_observations': [\n",
    "            'PDemo:ro.P-Demo_ct.InCaseTkn',\n",
    "            'PZip3Demo:ro.P-Zip3DemoNume_ct.InCaseTkn',\n",
    "            'PZip3Econ:ro.P-Zip3EconNume_ct.InCaseTkn',\n",
    "            'PZip3House:ro.P-Zip3HousingNume_ct.InCaseTkn',\n",
    "            'PZip3Social:ro.P-Zip3SocialNume_ct.InCaseTkn',\n",
    "            'RxInCase1:ro.Rx-InObs-CmpCate_ct.InCaseTkn',\n",
    "            'RxInCase2:ro.Rx-InObs-InsCate_ct.InCaseTkn',\n",
    "            'RxInCase4:ro.Rx-InObs-QuantNume_ct.InCaseTkn',\n",
    "            'RxInCase5:ro.Rx-InObs-ServiceCate_ct.InCaseTkn',\n",
    "            'RxInCase6:ro.Rx-InObs-SysCate_ct.InCaseTkn',\n",
    "            'RxInObsNum:ro.Rx-InObs_ct.RecNum',\n",
    "        ], \n",
    "        'name_CaseGamma': 'InputCatCOs',\n",
    "    },\n",
    "    'OutputY': {\n",
    "        'case_observations': [\n",
    "            'FutEduTknY:ro.EgmEdu-Af1W_ct.FutRxEduTkn',  \n",
    "        ],\n",
    "        'name_CaseGamma': 'LabelBinaryRxBtn',\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a498e5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.observer import get_fn_case_GammaFullInfo\n",
    "\n",
    "\n",
    "CFType_to_CaseFeatInfo = {}\n",
    "for CFType, Gamma_Config in Gamma_Configs.items():\n",
    "\n",
    "    if use_inference == True and 'output' in CFType.lower(): continue \n",
    "    \n",
    "    logger.info(f'============ CFType: {CFType} =============')\n",
    "    CaseFeatInfo = get_fn_case_GammaFullInfo(Gamma_Config, \n",
    "                                             base_config, \n",
    "                                             RecName_to_dsRec, \n",
    "                                             RecName_to_dsRecInfo, \n",
    "                                             df_case_learning,\n",
    "                                             use_CF_from_disk,\n",
    "                                             use_CO_from_disk)\n",
    "    CFType_to_CaseFeatInfo[CFType] = CaseFeatInfo\n",
    "    FnCaseFeatGamma = CaseFeatInfo['FnCaseFeatGamma']\n",
    "    batch_size = CaseFeatInfo.get('batch_size', 1000)\n",
    "    CaseFeatName = CaseFeatInfo['CaseFeatName']\n",
    "    for splitname, ds_caseset in ds_case_dict.items():\n",
    "        logger.info(f'----- splitname: {splitname} -----')\n",
    "        ds_caseset = ds_caseset.map(FnCaseFeatGamma, \n",
    "                                    batched = True, \n",
    "                                    batch_size= batch_size, \n",
    "                                    load_from_cache_file = False, \n",
    "                                    new_fingerprint = CaseFeatName + splitname.replace(':', '_'))\n",
    "        ds_case_dict[splitname] = ds_caseset\n",
    "        # logger.info(ds_caseset)\n",
    "    ######## save to cache file #########\n",
    "    # print(len(FnCaseFeatGamma.new_CFs))\n",
    "    if len(FnCaseFeatGamma.new_CFs) > 0 and use_CF_from_disk == True:\n",
    "        logger.info(f'----- Save CF {CaseFeatName}: to Cache File -----')\n",
    "        FnCaseFeatGamma.save_new_CFs_to_disk()\n",
    "    \n",
    "    for COName, FnCaseObsPhi in FnCaseFeatGamma.COName_to_FnCaseObsPhi.items():\n",
    "        if len(FnCaseObsPhi.new_COs) > 0 and use_CO_from_disk == True:\n",
    "            logger.info(f'----- Save CO {COName}: to Cache File -----')\n",
    "            FnCaseObsPhi.save_new_COs_to_disk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641757b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_case_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a096c80b",
   "metadata": {},
   "source": [
    "# Step 4: Entry - Put DP to Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa66156",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "entry_method_for_input = 'get_MLSparseMatrix_return_X'\n",
    "#############################################\n",
    "\n",
    "\n",
    "import inspect\n",
    "from recfldtkn.loadtools import convert_variables_to_pystirng\n",
    "\n",
    "def fn_entry_method_for_input(dataset, CaseFeatInfo):\n",
    "    from scipy.sparse import csr_matrix\n",
    "    import itertools\n",
    "    CF_vocab_input = CaseFeatInfo['CF_vocab']\n",
    "    tid2tkn = CF_vocab_input['input_ids']['tid2tkn']\n",
    "    num_features = len(tid2tkn)\n",
    "    col_indices = list(itertools.chain(*dataset['input_ids']))\n",
    "    row_indices = list(itertools.chain(*[[i] * len(x) \n",
    "                                         for i, x in enumerate(dataset['input_ids'])]))\n",
    "    data = list(itertools.chain(*dataset['input_wgts']))\n",
    "    sparse_matrix_value = (data, (row_indices, col_indices))\n",
    "    shape = (len(dataset), num_features)\n",
    "    X = csr_matrix(sparse_matrix_value, shape=shape)\n",
    "    return X\n",
    "\n",
    "fn_entry_method_for_input.fn_string = inspect.getsource(fn_entry_method_for_input)\n",
    "\n",
    "\n",
    "prefix = ['import pandas as pd', 'import numpy as np']\n",
    "fn_variables = [fn_entry_method_for_input]\n",
    "pycode = convert_variables_to_pystirng(fn_variables = fn_variables, prefix = prefix)\n",
    "pypath = os.path.join(SPACE['CODE_FN'], 'fn_learning', f'{entry_method_for_input}.py')\n",
    "print(pypath)\n",
    "if not os.path.exists(os.path.dirname(pypath)): os.makedirs(os.path.dirname(pypath))\n",
    "with open(pypath, 'w') as file: file.write(pycode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918535b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "entry_method_for_output = 'get_OutputBinaryLabel_return_Y'\n",
    "#############################################\n",
    "\n",
    "def fn_entry_method_for_output(dataset, CaseFeatInfo):\n",
    "    import numpy as np\n",
    "    Y_id = 0\n",
    "    CF_vocab_output = CaseFeatInfo['CF_vocab']\n",
    "    # Y_label = CF_vocab_output['labels']['tid2tkn'][Y_id]\n",
    "    assert len(CF_vocab_output['labels']['tid2tkn']) == 1\n",
    "    labels_wgts = dataset['label_wgts']\n",
    "    labels = np.array([i[Y_id] for i in labels_wgts])\n",
    "    return labels\n",
    "\n",
    "\n",
    "fn_entry_method_for_output.fn_string = inspect.getsource(fn_entry_method_for_output)\n",
    "\n",
    "prefix = ['import pandas as pd', 'import numpy as np']\n",
    "fn_variables = [fn_entry_method_for_output]\n",
    "pycode = convert_variables_to_pystirng(fn_variables = fn_variables, prefix = prefix)\n",
    "pypath = os.path.join(SPACE['CODE_FN'], 'fn_learning', f'{entry_method_for_output}.py')\n",
    "print(pypath)\n",
    "if not os.path.exists(os.path.dirname(pypath)): os.makedirs(os.path.dirname(pypath))\n",
    "with open(pypath, 'w') as file: file.write(pycode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12bccf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "entry_method_for_finaldata = 'tuple_XMatrix_and_YMatrix'\n",
    "#############################################\n",
    "\n",
    "def fn_entry_method_for_finaldata(dataset, \n",
    "                                  CFType_to_CaseFeatInfo,\n",
    "                                  fn_entry_method_for_input, \n",
    "                                  fn_entry_method_for_output = None,\n",
    "                                  use_inference = False, \n",
    "                                  ):\n",
    "    CaseFeatInfo = CFType_to_CaseFeatInfo['InputX']\n",
    "    X = fn_entry_method_for_input(dataset, CaseFeatInfo)\n",
    "\n",
    "    if use_inference == False:\n",
    "        CaseFeatInfo = CFType_to_CaseFeatInfo['OutputY']\n",
    "        Y = fn_entry_method_for_output(dataset, CaseFeatInfo)\n",
    "        results = {'X': X, 'Y': Y}\n",
    "    else:\n",
    "        results = {'X': X}\n",
    "    return results\n",
    "\n",
    "\n",
    "fn_entry_method_for_finaldata.fn_string = inspect.getsource(fn_entry_method_for_finaldata)\n",
    "\n",
    "prefix = ['import pandas as pd', 'import numpy as np']\n",
    "fn_variables = [fn_entry_method_for_finaldata]\n",
    "pycode = convert_variables_to_pystirng(fn_variables = fn_variables, prefix = prefix)\n",
    "pypath = os.path.join(SPACE['CODE_FN'], 'fn_learning', f'{entry_method_for_finaldata}.py')\n",
    "print(pypath)\n",
    "if not os.path.exists(os.path.dirname(pypath)): os.makedirs(os.path.dirname(pypath))\n",
    "with open(pypath, 'w') as file: file.write(pycode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8656af",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_args = {\n",
    "    'entry_method_for_input': entry_method_for_input, \n",
    "    'entry_method_for_output': entry_method_for_output,\n",
    "    'entry_method_for_finaldata': entry_method_for_finaldata,\n",
    "}\n",
    "\n",
    "entry_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6d701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.loadtools import fetch_entry_tools, load_module_variables\n",
    "\n",
    "def fetch_entry_tools(entry_args, SPACE):\n",
    "    tools = {}\n",
    "    for entry_name, entry_method in entry_args.items():\n",
    "        pypath = os.path.join(SPACE['CODE_FN'], 'fn_learning', f'{entry_method}.py')\n",
    "        module = load_module_variables(pypath)\n",
    "        # print([i for i in module.MetaDict])\n",
    "        tools[entry_name] = module.MetaDict['fn_' + entry_name]\n",
    "    return tools\n",
    "\n",
    "Entry_Tools = fetch_entry_tools(entry_args, SPACE)\n",
    "fn_entry_method_for_input = Entry_Tools['entry_method_for_input']\n",
    "fn_entry_method_for_output = Entry_Tools['entry_method_for_output']\n",
    "fn_entry_method_for_finaldata = Entry_Tools['entry_method_for_finaldata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc646c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c9bddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ds_case_dict[TrainSetName]\n",
    "result = fn_entry_method_for_finaldata(dataset, \n",
    "                                       CFType_to_CaseFeatInfo,\n",
    "                                       fn_entry_method_for_input, \n",
    "                                       fn_entry_method_for_output,\n",
    "                                       use_inference)\n",
    "\n",
    "X, Y = result['X'], result['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a85630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945459fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "ds_case_final_dict = {}\n",
    "for split, dataset in ds_case_dict.items():\n",
    "    dataset = fn_entry_method_for_finaldata(dataset, \n",
    "                                           CFType_to_CaseFeatInfo,\n",
    "                                           fn_entry_method_for_input, \n",
    "                                           fn_entry_method_for_output,\n",
    "                                           use_inference)\n",
    "    ds_case_final_dict[split] = dataset\n",
    "\n",
    "ds_case_final_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc3ab3d",
   "metadata": {},
   "source": [
    "# Step 5: Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039d5157",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "model_args = {\n",
    "    'algorithm': 'XGBClassifier',\n",
    "    'random_state': 42, \n",
    "    'max_depth': 5,\n",
    "}\n",
    "######################\n",
    "\n",
    "####################\n",
    "training_args = {\n",
    "    'n_estimators': 500, # num_boost_round\n",
    "    'learning_rate': 0.1, # eta\n",
    "    'objective': 'binary:logistic', \n",
    "    'early_stopping_rounds': 10,\n",
    "    'eval_metric': 'logloss',  \n",
    "}\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd550258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "train_set = ds_case_final_dict[TrainSetName]\n",
    "\n",
    "eval_sets = [(eval_set['X'], eval_set['Y']) for k, eval_set in ds_case_final_dict.items()]\n",
    "set_names = [k for k in ds_case_final_dict]\n",
    "\n",
    "# model\n",
    "algorithm = model_args['algorithm']\n",
    "if algorithm == 'XGBClassifier':\n",
    "    from xgboost import XGBClassifier \n",
    "    args = {k: v for k, v in model_args.items() if k != 'algorithm'}\n",
    "    model = XGBClassifier(**args, **training_args)\n",
    "else:\n",
    "    raise ValueError(f'not yet support {algorithm}')\n",
    "\n",
    "# training\n",
    "print(set_names)\n",
    "X, Y = train_set['X'], train_set['Y']\n",
    "model.fit(X, Y, eval_set = eval_sets, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e306ccf4",
   "metadata": {},
   "source": [
    "# Step 6: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581e8d9d",
   "metadata": {},
   "source": [
    "## In-Training Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864e7c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "evals_result = model.evals_result()\n",
    "results = [v for k, v in evals_result.items()]\n",
    "evals_result = dict(zip(set_names, results))\n",
    "[i for i in evals_result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbc0b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric = training_args['eval_metric']\n",
    "epochs = len(evals_result[set_names[0]][eval_metric])\n",
    "x_axis = range(0, epochs)\n",
    "fig, ax = plt.subplots()\n",
    "for splitname in evals_result:\n",
    "    ax.plot(x_axis, evals_result[splitname]['logloss'], label=splitname)\n",
    "ax.legend()\n",
    "plt.ylabel('Log Loss')\n",
    "plt.title('XGBoost Log Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4189f70",
   "metadata": {},
   "source": [
    "## Post-Training Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a20736",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_case_final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1652ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "# For binary classification, convert probabilities to binary predictions (0 or 1)\n",
    "# This threshold can be adjusted based on your requirements\n",
    "threshold = 0.1\n",
    "\n",
    "for EvalSetName in EvalSetNames:\n",
    "    print(EvalSetName)\n",
    "    dataset = ds_case_final_dict[EvalSetName]\n",
    "    X, y_real_label = dataset['X'], dataset['Y']\n",
    "    \n",
    "    y_real_label_mean = np.mean(y_real_label)\n",
    "    if y_real_label_mean == 0: \n",
    "        print(f\"Mean of Real Y: {y_real_label_mean} is 0\"); continue\n",
    "\n",
    "    y_pred_score = model.predict_proba(X)[:, 1] # 0, negative, 1, positive\n",
    "    y_pred_label = [1 if score > threshold else 0 for score in y_pred_score]\n",
    "    \n",
    "    assert len(y_real_label) == len(y_pred_label)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy  = round(accuracy_score(y_real_label, y_pred_label), 2)\n",
    "    precision = round(precision_score(y_real_label, y_pred_label), 2)\n",
    "    recall    = round(recall_score(y_real_label, y_pred_label), 2)\n",
    "    f1 = f1_score(y_real_label, y_pred_label)\n",
    "    roc_auc = roc_auc_score(y_real_label, y_pred_score)\n",
    "    fpr, tpr, thresholds = roc_curve(y_real_label, y_pred_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Print metrics\n",
    "    num_sample = len(y_pred_label)\n",
    "    postive_num = (np.array(y_pred_label) == 1).sum()\n",
    "    label1_num = (np.array(y_real_label) == 1).sum()\n",
    "    print(f\"Mean of Real Y Label: {y_real_label_mean} over {num_sample} num_sample\")\n",
    "    print(f\"Accuracy (TP + TN / TP + TN + FP + FN): {accuracy} over {num_sample} num_sample\")\n",
    "    print(f\"Precision (TP / TP + FP): {precision} over {postive_num} postive_num\") # the choosed postive. how many are correct.\n",
    "    print(f\"Recall (TP / TP + FN): {recall} over {label1_num} label1_num\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"ROC-AUC: {roc_auc}\")\n",
    "    \n",
    "    df = pd.DataFrame({'fpr': fpr, 'tpr': tpr, 'thresholds': thresholds})\n",
    "    print(df)\n",
    "\n",
    "    # Confusion matrix\n",
    "    # tn, fp, fn, tp = confusion_matrix(y_real_label, y_pred_label).ravel()\n",
    "\n",
    "    # # TPR and FPR\n",
    "    # if np.mean(y_pred_label) != 0 and np.mean(y_pred_label) != 1:\n",
    "    #     tpr = tp / (tp + fn)\n",
    "    #     fpr = fp / (fp + tn)\n",
    "    #     print('\\nthreshold:', threshold)\n",
    "    #     print(f\"True Positive Rate: {tpr}\")\n",
    "    #     print(f\"False Positive Rate: {fpr}\\n\")\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df_eval = pd.DataFrame({\n",
    "        'y_real_label': y_real_label,\n",
    "        'y_pred_score': y_pred_score.tolist()\n",
    "    })\n",
    "\n",
    "    # Sort the DataFrame based on the 'Predicted' column\n",
    "    df_eval = df_eval.sort_values(by='y_pred_score', ascending=False) \n",
    "\n",
    "    # For each percentile: i% to i + 1 %, report its the ratio of that y label is 1.\n",
    "    # Calculate the percentile for each row based on the 'Predicted' column\n",
    "    Group_Num = 100\n",
    "    df_eval['y_pred_score_Group'] = pd.qcut(df_eval['y_pred_score'], Group_Num, labels=False)\n",
    "\n",
    "    # Calculate the ratio of y label being 1 for each percentile\n",
    "    Group_to_GroupInfo = {}\n",
    "    Group_Values = sorted(list(df_eval['y_pred_score_Group'].unique()))\n",
    "    for group_value in Group_Values:\n",
    "        df_eval_predscore_group = df_eval[df_eval['y_pred_score_Group'] == group_value]\n",
    "        y_real_label_rate = np.mean(df_eval_predscore_group['y_real_label'])\n",
    "\n",
    "        GroupInfo = {\n",
    "            'y_real_label_rate': y_real_label_rate,\n",
    "            'y_pred_score_mean': np.mean(df_eval_predscore_group['y_pred_score']),\n",
    "            'y_pred_score_max': np.max(df_eval_predscore_group['y_pred_score']),\n",
    "            'y_pred_score_min': np.min(df_eval_predscore_group['y_pred_score']),\n",
    "            'group_size': len(df_eval_predscore_group),\n",
    "        }\n",
    "        GroupInfo = {k: round(v, 4) for k, v in GroupInfo.items()}  \n",
    "        Group_to_GroupInfo[group_value] = GroupInfo\n",
    "    \n",
    "    # # Print the ratios for each percentile\n",
    "    for group_value, GroupInfo in Group_to_GroupInfo.items():\n",
    "        print(f\"y_pred_score_Group-{group_value}: {GroupInfo}\")\n",
    "\n",
    "    # Plot the ratios\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for label in GroupInfo:\n",
    "        if label == 'group_size': continue\n",
    "        plt.plot(Group_to_GroupInfo.keys(), \n",
    "                 [GroupInfo[label] for GroupInfo in Group_to_GroupInfo.values()],\n",
    "                 # marker='o',\n",
    "                 label = label)\n",
    "    plt.xlabel('y_pred_score_Group')\n",
    "    plt.ylabel('Ratio of y label being 1')\n",
    "    plt.title('Ratio of y label being 1 for each y_pred_score_Group')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
    "            label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    # Select thresholds to annotate on the curve\n",
    "    # You can choose specific thresholds that are of interest or select them based on criteria\n",
    "    indices_to_annotate = [len(thresholds) // 3 * 1, len(thresholds) // 2, len(thresholds) // 3 * 2, len(thresholds) - 1]  # Example: start, middle, end\n",
    "    for i in indices_to_annotate:\n",
    "        plt.annotate(f'Threshold={thresholds[i]:.2f}', (fpr[i], tpr[i]),\n",
    "                    textcoords=\"offset points\", xytext=(-10,-10), ha='center')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff753654",
   "metadata": {},
   "source": [
    "# Step 7: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8a9300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "###############\n",
    "TASK = SPACE['MODEL_TASK'].split('_')[-1]\n",
    "date = datetime.now().strftime(\"%Y.%m.%d\")\n",
    "model_checkpoint_name = f'{TASK}-{date}-naive-XGBoost'\n",
    "print(model_checkpoint_name)\n",
    "###############\n",
    "\n",
    "model_repo_local = '../model/model_phase1/v0.1/'\n",
    "\n",
    "\n",
    "model_checkpoint_path = os.path.join(model_repo_local, model_checkpoint_name)\n",
    "if not os.path.exists(model_checkpoint_path): os.makedirs(model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a9e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "LoadFnMethod = 'LoadXGBoost'\n",
    "####################################\n",
    "\n",
    "\n",
    "import inspect\n",
    "\n",
    "def save_model(model, model_checkpoint_path):\n",
    "    if not os.path.exists(model_checkpoint_path): os.makedirs(model_checkpoint_path)\n",
    "    model_path = os.path.join(model_checkpoint_path, 'model.xgb')\n",
    "    model.save_model(model_path)\n",
    "\n",
    "def load_model(model_checkpoint_path):\n",
    "    model_path = os.path.join(model_checkpoint_path, 'model.xgb')\n",
    "    import xgboost as xgb\n",
    "    model = xgb.XGBClassifier()\n",
    "    model.load_model(model_path)\n",
    "    return model\n",
    "\n",
    "def model_inference(model, inputs):\n",
    "    X = inputs['X']\n",
    "    y_pred_scores = model.predict_proba(X)[:, 1]\n",
    "    # assert len(y_pred_scores) == 1\n",
    "    y_pred_score = y_pred_scores[0]\n",
    "    return y_pred_score\n",
    "\n",
    "save_model.fn_string = inspect.getsource(save_model)\n",
    "load_model.fn_string = inspect.getsource(load_model)\n",
    "model_inference.fn_string = inspect.getsource(model_inference)\n",
    "prefix = ['import pandas as pd', 'import numpy as np', 'import os']\n",
    "fn_variables = [save_model, load_model, model_inference]\n",
    "pycode = convert_variables_to_pystirng(fn_variables = fn_variables, prefix = prefix)\n",
    "pypath = os.path.join(SPACE['CODE_FN'], 'fn_io', f'{LoadFnMethod}.py')\n",
    "print(pypath)\n",
    "if not os.path.exists(os.path.dirname(pypath)): os.makedirs(os.path.dirname(pypath))\n",
    "with open(pypath, 'w') as file: file.write(pycode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc68c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662142de",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, model_checkpoint_path)\n",
    "model = load_model(model_checkpoint_path)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fd0d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_args = {\n",
    "    'TriggerCaseMethod': TriggerCaseMethod,\n",
    "    'cohort_label_list': cohort_label_list,\n",
    "    'Trigger2LearningMethods': Trigger2LearningMethods,\n",
    "}\n",
    "case_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f99ee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_args = {\n",
    "    'RANDOM_SEED': RANDOM_SEED,\n",
    "    'downsample_ratio': downsample_ratio,\n",
    "    'out_ratio': out_ratio,\n",
    "    'test_ratio': test_ratio,\n",
    "    'valid_ratio': valid_ratio,\n",
    "    'SplitMethod': SplitMethod,\n",
    "}\n",
    "split_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d4b8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_args = {\n",
    "    'TrainSetName': TrainSetName,\n",
    "    'EvalSetNames': EvalSetNames,\n",
    "    'SubGroupFilterMethod': SubGroupFilterMethod,\n",
    "}\n",
    "set_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0328deb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoint_args = Gamma_Configs\n",
    "datapoint_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb90d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcaf54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd396ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args = {\n",
    "    'case_args': case_args,\n",
    "    'split_args': split_args,\n",
    "    'set_args': set_args,\n",
    "    'datapoint_args': datapoint_args,\n",
    "    'entry_args': entry_args,\n",
    "}\n",
    "\n",
    "pprint(data_args, sort_dicts=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c834bf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess dp to models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8a7a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1487ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fc84b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "io_args = {\n",
    "    'model_checkpoint_name': model_checkpoint_name,\n",
    "    'LoadFnMethod': LoadFnMethod,\n",
    "}\n",
    "io_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebb1ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.pipeline_model import load_complete_PipelineInfo\n",
    "\n",
    "\n",
    "use_inference = True\n",
    "PipelineInfo = load_complete_PipelineInfo(datapoint_args, base_config, use_inference)\n",
    "PipelineInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1097de",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_inference = False\n",
    "PipelineInfo = load_complete_PipelineInfo(datapoint_args, base_config, use_inference)\n",
    "PipelineInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa48aa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933d0552",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "New_SPACE = {\n",
    "    # 'WORKSPACE_PATH': WORKSPACE_PATH, \n",
    "    'DATA_EXTERNAL': f'external',\n",
    "    'CODE_FN': f'pipeline', \n",
    "    'CODE_RFT': f'pipeline',\n",
    "    'DATA_RAW': None ,\n",
    "    'DATA_RFT': None,\n",
    "    'DATA_CaseObs': None,\n",
    "    'DATA_CaseFeat': None,\n",
    "    'DATA_CaseSet': None,\n",
    "    'DATA_TASK': None,\n",
    "    'MODEL_TASK': None, \n",
    "    'TaskName': None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6597cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_args = {\n",
    "    'data_args': data_args,\n",
    "    'model_args': model_args,\n",
    "    'training_args': training_args,\n",
    "    'io_args': io_args,\n",
    "    'SPACE': New_SPACE,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cc5355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "# checkpioint_args\n",
    "\n",
    "#### model\n",
    "model_path = os.path.join(model_checkpoint_path, 'model.xgb')\n",
    "LoadFnMethod = LoadFnMethod\n",
    "\n",
    "#### checkpoint_args_path\n",
    "path = os.path.join(model_checkpoint_path, 'checkpoint_args.json')\n",
    "with open(path, 'w') as f:\n",
    "    json.dump(checkpoint_args, f, indent=4)\n",
    "\n",
    "\n",
    "#### CFType_to_CFVocab\n",
    "CFType_to_CFVocab = {k: v['CF_vocab'] for k, v in CFType_to_CaseFeatInfo.items()}\n",
    "path = os.path.join(model_checkpoint_path, 'CFType_to_CFVocab.json')\n",
    "with open(path, 'w') as f:\n",
    "    json.dump(CFType_to_CFVocab, f, indent=4)\n",
    "\n",
    "\n",
    "#### pipeline \n",
    "pipeline_folder = os.path.join(model_checkpoint_path, 'pipeline')\n",
    "if os.path.exists(pipeline_folder):\n",
    "    shutil.rmtree(pipeline_folder)\n",
    "shutil.copytree(SPACE['CODE_FN'], pipeline_folder)\n",
    "print(pipeline_folder)\n",
    "\n",
    "\n",
    "#### external \n",
    "external_folder = os.path.join(model_checkpoint_path, 'external')\n",
    "if os.path.exists(external_folder):\n",
    "    shutil.rmtree(external_folder)\n",
    "shutil.copytree(SPACE['DATA_EXTERNAL'], external_folder)\n",
    "print(external_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4235047a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
